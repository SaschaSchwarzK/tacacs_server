from __future__ import annotations

import hashlib
import json
import os
import platform
import shutil
import socket
import uuid
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

from tacacs_server.config.config import TacacsConfig
from tacacs_server.utils.logger import get_logger

from .archive_utils import extract_tarball as extract_tar
from .database_utils import (
    count_database_records as db_count_tables,
)
from .database_utils import (
    export_database_with_retry as db_export,
)
from .database_utils import (
    import_database as db_import,
)
from .database_utils import (
    verify_database_integrity as db_verify,
)
from .destinations import create_destination
from .encryption import BackupEncryption, decrypt_file, encrypt_file
from .execution_store import BackupExecutionStore
from .scheduler import BackupScheduler

_logger = get_logger("tacacs_server.backup.service", component="backup")


class BackupService:
    """Service for managing backups."""
    
    def __init__(self, config, execution_store):
        """Initialize the backup service.
        
        Args:
            config: Configuration object
            execution_store: Store for backup executions
        """
        self.config = config
        self.execution_store = execution_store
        
        try:
            bcfg = getattr(self.config, "get_backup_config", None)
            if callable(bcfg):
                temp_dir = bcfg().get("temp_directory") or "data/backup_temp"
            else:
                temp_dir = "data/backup_temp"
        except Exception:
            temp_dir = "data/backup_temp"
        self.temp_dir = Path(str(temp_dir))
        self._ensure_temp_dir()
        self.instance_name = (
            self.config.config_store.get_instance_name()
            if self.config.config_store
            else "tacacs-server"
        )
        # Initialize scheduler (not started by default)
        try:
            self.scheduler = BackupScheduler(self)
        except Exception:
            self.scheduler = None  # type: ignore[assignment]
        self.instance_id = (
            self.config.config_store.get_metadata("instance_id")
            if self.config.config_store
            else None
        )

    def _ensure_temp_dir(self) -> None:
        self.temp_dir.mkdir(parents=True, exist_ok=True)

    # --- helpers ---
    def _export_database(self, src_path: str, dst_path: str) -> None:
        """Export a SQLite database from src_path to dst_path with verification.
        
        Args:
            src_path: Source database path
            dst_path: Destination path for the exported database
        """
        try:
            db_export(src_path, dst_path)
            if not os.path.exists(dst_path):
                raise RuntimeError(f"Export failed: {dst_path} was not created")
        except Exception as e:
            _logger.error(f"Database export failed: {e}")
            raise

    def _create_manifest(self, backup_type: str, triggered_by: str) -> dict:
        """Create a backup manifest.
        
        Args:
            backup_type: Reason/type for backup (e.g. "manual", "scheduled")
            triggered_by: Initiator for audit trail
            
        Returns:
            Manifest dictionary suitable for JSON serialization
        """
        return {
            "backup_metadata": {
                "instance_id": self.instance_id,
                "instance_name": self.instance_name,
                "hostname": socket.gethostname(),
                "backup_type": backup_type,
                "timestamp_utc": datetime.now(UTC).isoformat(),
                "triggered_by": triggered_by,
                "backup_version": "1.0",
            },
            "system_info": {
                "server_version": self._get_server_version(),
                "python_version": platform.python_version(),
                "platform": platform.platform(),
            },
            "config_info": {
                "config_source": "url" if self.config.is_url_config() else "file",
                "config_hash": self._hash_dict(self.config._export_full_config()),
                "has_overrides": len(getattr(self.config, "overridden_keys", {}) or {})
                > 0,
            },
            "contents": [],
            "total_size_bytes": 0,
            # Encryption fields (populated when encryption is applied)
            "encrypted": False,
            "encryption_algorithm": None,
            "encryption_metadata": None,
        }

        for filename in os.listdir(backup_dir):
            if filename == "manifest.json":
                continue
            filepath = os.path.join(backup_dir, filename)
            if not os.path.isfile(filepath):
                continue
            try:
                file_size = os.path.getsize(filepath)
            except Exception:
                file_size = 0
            checksum = self._calculate_sha256(filepath)
            ftype = self._classify_file(filename)
            entry: dict[str, Any] = {
                "file": filename,
                "size_bytes": file_size,
                "checksum_sha256": checksum,
                "type": ftype,
            }
            if ftype == "database":
                try:
                    entry["records"] = sum(db_count_tables(filepath).values())
                except Exception:
                    entry["records"] = 0
            manifest["contents"].append(entry)
            manifest["total_size_bytes"] += file_size

        return manifest

    @staticmethod
    def _calculate_sha256(filepath: str) -> str:
        h = hashlib.sha256()
        try:
            with open(filepath, "rb") as f:
                for chunk in iter(lambda: f.read(8192), b""):
                    h.update(chunk)
            return h.hexdigest()
        except Exception:
            return ""

    @staticmethod
    def _hash_dict(d: dict) -> str:
        try:
            data = json.dumps(d, sort_keys=True, separators=(",", ":")).encode("utf-8")
            return hashlib.sha256(data).hexdigest()
        except Exception:
            return ""

    @staticmethod
    def _classify_file(filename: str) -> str:
        name = filename.lower()
        if name.endswith(".db"):
            return "database"
        if name in ("tacacs.conf", "config_export.json"):
            return "config"
        if name == "manifest.json":
            return "metadata"
        return "data"

    @staticmethod
    def _count_db_records(db_path: str) -> int:
        try:
            return sum(db_count_tables(db_path).values())
        except Exception:
            return 0

    @staticmethod
    def _get_server_version() -> str:
        try:
            from tacacs_server import __version__

            return str(__version__)
        except Exception:
            return "unknown"

    def _create_tarball(self, src_dir: str, archive_path: str) -> int:
        """Create a tarball from the source directory.
        
        Args:
            src_dir: Source directory to compress
            archive_path: Path where the tarball will be created
            
        Returns:
            int: Size of the created tarball in bytes
        """
        try:
            compression_level = getattr(self.config, "get_backup_config", lambda: {})().get(
                "compression_level", 6
            )
        except Exception:
            compression_level = 6
            
        # Ensure directory exists
        Path(archive_path).parent.mkdir(parents=True, exist_ok=True)
        
        # Create the tarball
        with tarfile.open(archive_path, "w:gz", compresslevel=int(compression_level)) as tar:
            tar.add(src_dir, arcname=".")
        return _os.path.getsize(archive_path)

    def _upload_with_progress(
        self,
        local_path: str,
        destination,
        remote_filename: str,
        execution_id: str,
    ) -> str:
        """Upload with periodic progress logs (best-effort callback).
        
        Args:
            local_path: Path to the local file to upload
            destination: Destination object with upload method
            remote_filename: Filename to use at the destination
            execution_id: ID of the backup execution
            
        Returns:
            str: Remote path where the file was uploaded
        """
        file_size = os.path.getsize(local_path)
        last_log_time = 0
        bytes_uploaded = 0
        
        def progress_callback(chunk):
            nonlocal bytes_uploaded, last_log_time
            bytes_uploaded += len(chunk)
            
            # Log progress at most once per second
            current_time = time.time()
            if current_time - last_log_time >= 1.0:  # Log at most once per second
                last_log_time = current_time
                percent = (bytes_uploaded / file_size) * 100 if file_size > 0 else 0
                try:
                    _logger.info(
                        "Backup upload progress: %.1f%% (%s/%s)",
                        percent,
                        humanize.naturalsize(bytes_uploaded, binary=True),
                        humanize.naturalsize(file_size, binary=True),
                        extra={
                            "event": "backup_upload_progress",
                            "execution_id": execution_id,
                            "bytes_uploaded": bytes_uploaded,
                            "total_bytes": file_size,
                            "percent_complete": round(percent, 2),
                        },
                    )
                except Exception as e:
                    _logger.warning("Failed to log upload progress: %s", e)
        
        # Try to pass progress_callback if destination supports it
        try:
            if hasattr(destination, 'upload'):
                return destination.upload(
                    local_path=local_path,
                    remote_filename=remote_filename,
                    progress_callback=progress_callback,
                )
            elif hasattr(destination, 'upload_backup'):
                return destination.upload_backup(
                    local_path=local_path,
                    remote_filename=remote_filename,
                    progress_callback=progress_callback
                )
            else:
                # Fallback to simple upload if neither method is available
                _logger.warning("Destination doesn't support progress callbacks, using simple upload")
                if hasattr(destination, 'upload'):
                    return destination.upload(local_path, remote_filename)
                else:
                    return destination.upload_backup(local_path, remote_filename)
                    
        except Exception as e:
            _logger.error("Upload failed: %s", str(e))
            raise

    def execute_backup(
        self,
        destination_id: str,
        triggered_by: str,
        backup_type: str = "manual",
        execution_id: str | None = None,
    ) -> str:
        """Execute a backup operation.
        
        Args:
            destination_id: ID of the destination where the backup will be stored
            triggered_by: Identifier for who/what triggered the backup
            backup_type: Type of backup (e.g., 'manual', 'scheduled')
            execution_id: Optional ID for this backup execution (will generate one if not provided)
            
        Returns:
            str: The execution ID of the backup
            
        Raises:
            ValueError: If any of the input parameters are invalid
        """
        if not isinstance(destination_id, str) or not destination_id.strip():
            raise ValueError("destination_id must be a non-empty string")
        if not isinstance(triggered_by, str) or not triggered_by.strip():
            raise ValueError("triggered_by must be a non-empty string")
        if not isinstance(backup_type, str) or not backup_type.strip():
            raise ValueError("backup_type must be a non-empty string")
            
        execution_id = execution_id or str(uuid.uuid4())
        backup_dir = os.path.join(str(self.temp_dir), execution_id)
        archive_path = None
        t0 = datetime.now(UTC)

        try:
            # Step 1: Initialize execution tracking
            self.execution_store.create_execution(
                execution_id=execution_id,
                destination_id=destination_id,
                triggered_by=triggered_by,
            )
            try:
                _logger.info(
                    json.dumps(
                        {
                            "event": "backup_started",
                            "execution_id": execution_id,
                            "destination_id": destination_id,
                            "triggered_by": triggered_by,
                            "backup_type": backup_type,
                        }
                    )
                )
            except Exception:
                pass

            # Step 2: Create temporary workspace
            os.makedirs(backup_dir, exist_ok=True)

            # Step 3: Export all databases
            databases_to_backup = [
                ("config_overrides.db", "data/config_overrides.db"),
                ("devices.db", self.config.get_device_store_config()["database"]),
                ("local_auth.db", self.config.get_local_auth_db()),
                (
                    "tacacs_accounting.db",
                    self.config.get_database_config()["accounting_db"],
                ),
                (
                    "metrics_history.db",
                    self.config.get_database_config()["metrics_history_db"],
                ),
                ("audit_trail.db", self.config.get_database_config()["audit_trail_db"]),
            ]
            for backup_name, source_path in databases_to_backup:
                try:
                    if source_path and os.path.exists(source_path):
                        self._export_database(
                            source_path, os.path.join(backup_dir, backup_name)
                        )
                except Exception as exc:
                    _logger.warning(
                        "backup_db_export_failed", db=backup_name, error=str(exc)
                    )

            # Step 4: Export configuration
            try:
                if not self.config.is_url_config() and getattr(
                    self.config, "config_file", None
                ):
                    if os.path.exists(self.config.config_file):
                        shutil.copy2(
                            self.config.config_file,
                            os.path.join(backup_dir, "tacacs.conf"),
                        )
            except Exception as exc:
                _logger.warning("backup_config_copy_failed", error=str(exc))

            config_dict = self.config._export_full_config()
            with open(
                os.path.join(backup_dir, "config_export.json"), "w", encoding="utf-8"
            ) as f:
                json.dump(config_dict, f, indent=2)

            # Step 5: Create manifest
            manifest = self._create_manifest(backup_dir, backup_type, triggered_by)
            with open(
                os.path.join(backup_dir, "manifest.json"), "w", encoding="utf-8"
            ) as f:
                json.dump(manifest, f, indent=2)

            # Step 6: Create compressed archive
            timestamp = datetime.now(UTC).strftime("%Y%m%d-%H%M%S")
            filename = f"backup-{self.instance_name}-{timestamp}-{backup_type}.tar.gz"
            archive_path = os.path.join(str(self.temp_dir), filename)
            self._create_tarball(backup_dir, archive_path)

            # Debug: Log the config object structure
            _logger.info(f"Config object type: {type(self.config).__name__}")
            if hasattr(self.config, '__dict__'):
                _logger.info(f"Config attributes: {', '.join([k for k in dir(self.config) if not k.startswith('_')])}")
            
            # Get encryption settings from config
            encryption_enabled = False
            passphrase_cfg = None
            
            # Try to get config from different possible locations
            try:
                # First try the backup config section
                bcfg = getattr(self.config, "get_backup_config", None)
                if callable(bcfg):
                    b = bcfg() or {}
                    _logger.info(f"Backup config from get_backup_config(): {b}")
                    encryption_enabled = b.get("encryption_enabled", False)
                    if isinstance(encryption_enabled, str):
                        encryption_enabled = encryption_enabled.lower() == "true"
                    passphrase_cfg = b.get("encryption_passphrase")
                
                # If not found, try direct config access
                if not encryption_enabled or not passphrase_cfg:
                    try:
                        if hasattr(self.config, 'config'):
                            _logger.info(f"self.config.config type: {type(self.config.config).__name__}")
                            if hasattr(self.config.config, 'get'):
                                backup_section = self.config.config.get('backup', {})
                                _logger.info(f"Backup section from config: {backup_section}")
                                if not encryption_enabled:
                                    val = backup_section.get('encryption_enabled', False)
                                    if isinstance(val, str):
                                        encryption_enabled = val.lower() == 'true'
                                    else:
                                        encryption_enabled = bool(val)
                                if not passphrase_cfg:
                                    passphrase_cfg = backup_section.get('encryption_passphrase')
                    except Exception as e:
                        _logger.warning(f"Error reading direct config: {e}")
                
                _logger.info(f"Final encryption settings - enabled: {encryption_enabled}, passphrase: {'***' if passphrase_cfg else 'None'}")
                
                # If encryption is enabled but no passphrase, log a warning
                if encryption_enabled and not passphrase_cfg:
                    _logger.warning("Encryption enabled but no passphrase configured")
                    
            except Exception as e:
                _logger.warning(f"Error getting encryption settings: {e}")
                _logger.warning(f"Config dump: {self.config.__dict__ if hasattr(self.config, '__dict__') else 'No __dict__'}")
                encryption_enabled = False
            if encryption_enabled:
                if not passphrase_cfg:
                    raise ValueError("Encryption enabled but no passphrase configured")
                try:
                    _logger.info(
                        json.dumps(
                            {
                                "event": "backup_encrypting",
                                "execution_id": execution_id,
                                "file": filename,
                            }
                        )
                    )
                except Exception:
                    pass

                encrypted_path = archive_path + ".enc"
                enc_info = BackupEncryption.encrypt_file(
                    archive_path, encrypted_path, passphrase_cfg
                )

                # Update manifest with encryption info
                try:
                    manifest["encrypted"] = True
                    encryption_algorithm = "Fernet-AES128-CBC"
                    manifest["encryption_algorithm"] = encryption_algorithm
                    manifest["original_checksum"] = enc_info.get("original_checksum")
                    manifest["encryption_metadata"] = {
                        "salt_hex": enc_info.get("salt_hex"),
                        "encrypted_size_bytes": enc_info.get("encrypted_size"),
                    }
                    # Also track compressed size pre-encryption for reporting
                    manifest["compressed_size_bytes"] = os.path.getsize(archive_path)
                except Exception:
                    pass

                # Replace archive with encrypted version
                try:
                    os.remove(archive_path)
                except Exception:
                    pass
                archive_path = encrypted_path
                # Ensure filename has .enc extension
                if not filename.endswith('.enc'):
                    filename = f"{filename}.enc"

                try:
                    _logger.info(
                        json.dumps(
                            {
                                "event": "backup_encrypted",
                                "execution_id": execution_id,
                                "original_size": enc_info.get("original_size"),
                                "encrypted_size": enc_info.get("encrypted_size"),
                                "size_increase_percent": round(
                                    (
                                        enc_info.get("encrypted_size", 0)
                                        / max(1, enc_info.get("original_size", 1))
                                        - 1
                                    )
                                    * 100,
                                    2,
                                ),
                            }
                        )
                    )
                except Exception:
                    pass

            # Step 7: Upload to destination
            dest_config = self.execution_store.get_destination(destination_id)
            if not dest_config:
                raise ValueError("Destination not found")
            destination = create_destination(
                dest_config["type"], json.loads(dest_config["config_json"])
            )
            ok, msg = destination.test_connection()
            if not ok:
                raise RuntimeError(f"Destination test failed: {msg}")

            # Update filename to include .enc extension if encrypted
            if encryption_enabled and not filename.endswith('.enc'):
                filename = f"{filename}.enc"
                
            remote_path = f"{self.instance_name}/{backup_type}/{filename}"
            uploaded_path = self._upload_with_progress(
                archive_path, destination, remote_path, execution_id
            )

            # Step 8: Update execution record
            archive_size = (
                os.path.getsize(archive_path) if os.path.exists(archive_path) else 0
            )
            
            # Ensure backup_filename has .enc extension if encrypted
            backup_filename = filename
            if encryption_enabled and not backup_filename.endswith('.enc'):
                backup_filename = f"{backup_filename}.enc"
                
            self.execution_store.update_execution(
                execution_id,
                backup_filename=backup_filename,
                backup_path=uploaded_path,
                status="completed",
                size_bytes=manifest["total_size_bytes"],
                compressed_size_bytes=archive_size,
                files_included=len(manifest["contents"]),
                completed_at=datetime.now(UTC).isoformat(),
                manifest_json=json.dumps(manifest),
            )
            self.execution_store.set_last_backup(destination_id, status="success")
            try:
                dur = (datetime.now(UTC) - t0).total_seconds()
                size = int(
                    manifest.get("total_size_bytes", archive_size) or archive_size
                )
                comp = int(archive_size)
                ratio = (comp / size) if size else None
                _logger.info(
                    json.dumps(
                        {
                            "event": "backup_completed",
                            "execution_id": execution_id,
                            "duration_seconds": dur,
                            "size_bytes": size,
                            "compressed_size_bytes": comp,
                            "compression_ratio": ratio,
                            "files_count": int(len(manifest.get("contents", []))),
                        }
                    )
                )
            except Exception:
                pass
            return execution_id
        except Exception as exc:
            try:
                _logger.exception(
                    json.dumps(
                        {
                            "event": "backup_failed",
                            "execution_id": execution_id,
                            "error": str(exc),
                            "error_type": type(exc).__name__,
                        }
                    )
                )
            except Exception:
                pass
            try:
                self.execution_store.update_execution(
                    execution_id, status="failed", error_message=str(exc)
                )
                self.execution_store.set_last_backup(destination_id, status="failed")
            except Exception:
                pass
            return execution_id
        finally:
            # Step 9: Cleanup
            try:
                if backup_dir and os.path.isdir(backup_dir):
                    shutil.rmtree(backup_dir, ignore_errors=True)
            except Exception:
                pass
            try:
                if archive_path and os.path.exists(archive_path):
                    os.remove(archive_path)
            except Exception:
                pass
            # Step 10: Apply retention policy (best-effort)
            try:
                dest_config = self.execution_store.get_destination(destination_id)
                if dest_config:
                    destination = create_destination(
                        dest_config["type"], json.loads(dest_config["config_json"])
                    )
                    # Prefer advanced retention strategy if configured
                    strat = str(dest_config.get("retention_strategy", "simple")).lower()
                    cfg_raw = dest_config.get("retention_config_json") or "{}"
                    try:
                        retention_cfg = (
                            json.loads(cfg_raw)
                            if isinstance(cfg_raw, str)
                            else (cfg_raw or {})
                        )
                    except Exception:
                        retention_cfg = {}
                    try:
                        from tacacs_server.backup.retention import (
                            RetentionRule as _Rule,
                            RetentionStrategy as _Strat,
                        )

                        rule = _Rule(strategy=_Strat(strat), **retention_cfg)
                        destination.apply_retention_policy(retention_rule=rule)
                    except Exception:
                        # Fallback to days if strategy invalid
                        rd = int(dest_config.get("retention_days", 30))
                        destination.apply_retention_policy(retention_days=rd)
            except Exception:
                pass

    def restore_backup(
        self,
        source_path: str,
        destination_id: str | None = None,
        components: list[str] | None = None,
    ) -> tuple[bool, str]:
        """
            return False, "Invalid source_path"

        allowed = {"config", "devices", "users", "accounting", "metrics", "audit"}
        if components is not None and not all(c in allowed for c in components):
            return False, f"Invalid component. Must be one of: {', '.join(allowed)}"

        # Step 2: If remote, download first (handles progress)
        if destination_id:
            dest = self.execution_store.get_destination(destination_id)
            if not dest:
                return False, f"Destination {destination_id} not found"

            _logger.info(
                json.dumps(
                    {
                        "event": "restore_downloading",
                        "source": source_path,
                        "destination_id": destination_id,
                    }
                )
            )
            try:
                local_archive = os.path.join(
                    str(self.temp_dir), os.path.basename(source_path)
                )
                dest.download(source_path, local_archive)
                _logger.info(
                    json.dumps(
                        {
                            "event": "restore_downloaded",
                            "source": source_path,
                            "size_bytes": os.path.getsize(local_archive),
                        }
                    )
                )
            except Exception as e:
                _logger.error(
                    json.dumps(
                        {
                            "event": "restore_download_failed",
                            "source": source_path,
                            "error": str(e),
                        }
                    )
                )
                return False, f"Download failed: {e}"
        else:
            local_archive = source_path

        # Handle encrypted backups
        is_encrypted = local_archive.endswith(".enc")
        archive_for_extract = local_archive

        if is_encrypted:
            # Get the passphrase from config
            try:
                bcfg = getattr(self.config, "get_backup_config", None)
                b = bcfg() if callable(bcfg) else {}
            except Exception as e:
                _logger.warning(f"Failed to get backup config: {e}")
                b = {}
            
            passphrase = b.get("encryption_passphrase")
            if not passphrase:
                return False, "Backup is encrypted but no passphrase configured"

            _logger.info(
                json.dumps({"event": "restore_decrypting", "source": source_path})
            )

            # Quick verification before full decrypt
            if not BackupEncryption.verify_passphrase(str(local_archive), passphrase):
                return False, "Incorrect encryption passphrase"

            # Decrypt to temp file
            decrypted_path = os.path.join(
                str(self.temp_dir), 
                f"decrypted_{os.path.basename(local_archive)[:-4]}"  # Remove .enc
            )
            
            _logger.info(f"Decrypting {local_archive} to {decrypted_path}")
            success = BackupEncryption.decrypt_file(
                str(local_archive), decrypted_path, passphrase
            )
            if not success:
                return False, "Decryption failed - file may be corrupted"
                
            _logger.info(
                json.dumps({"event": "restore_decrypted", "source": source_path})
            )
            
            # Use the decrypted file for extraction
            archive_for_extract = decrypted_path

        # Create restore directory
        restore_dir = os.path.join(str(self.temp_dir), f"restore_{uuid.uuid4()}")
        os.makedirs(restore_dir, exist_ok=True)
        
        # Extract the archive (either decrypted or original)
        try:
            restore_root = self._extract_tarball(archive_for_extract, restore_dir)
        except Exception as e:
            # Clean up decrypted file if it exists
            if is_encrypted and os.path.exists(archive_for_extract):
                try:
                    os.remove(archive_for_extract)
                except Exception:
                    pass
            return False, f"Failed to extract archive: {str(e)}"

        manifest_path = os.path.join(restore_root, "manifest.json")
        if not os.path.exists(manifest_path):
            # Clean up decrypted file if it exists
            if is_encrypted and os.path.exists(archive_for_extract):
                try:
                    os.remove(archive_for_extract)
                except Exception:
                    pass
            return False, "Manifest not found in backup"
            
        with open(manifest_path, encoding="utf-8") as f:
            manifest = json.load(f)

        # Verify checksums of all files in the backup
        for file_entry in manifest.get("contents", []):
            rel = file_entry.get("file") or file_entry.get("path")
            if not rel:
                continue
            fp = os.path.join(restore_root, rel)
            if os.path.isfile(fp):
                actual = self._calculate_sha256(fp)
                if (
                    actual
                    and file_entry.get("checksum_sha256")
                    and actual != file_entry.get("checksum_sha256")
                ):
                    # Clean up decrypted file if it exists
                    if is_encrypted and os.path.exists(archive_for_extract):
                        try:
                            os.remove(archive_for_extract)
                        except Exception:
                            pass
                    return False, f"Checksum mismatch for {rel}"

        # Step 3: Safety backup of current state (best-effort)
        try:
            emergency_dest_id = (
                destination_id or self._pick_fallback_destination_id()
            )
            if emergency_dest_id:
                emergency_exec_id = self.execute_backup(
                    destination_id=emergency_dest_id,
                    triggered_by="system:pre-restore",
                    backup_type="emergency",
                )
            else:
                _logger.warning("no_destination_for_emergency_backup")
        except Exception as e:
            _logger.warning(f"emergency_backup_failed: {str(e)}")

        # Step 4: Stop server components - handled by caller; log warning
        _logger.warning("server_restart_required_after_restore")

        # Step 5: Restore selected components
        if components is None:
            components = [
                "config",
                "devices",
                "users",
                "accounting",
                "metrics",
                "audit",
            ]

        db_map = {
            "devices": (
                os.path.join(restore_root, "devices.db"),
                self.config.get_device_store_config()["database"],
            ),
            "users": (
                os.path.join(restore_root, "local_auth.db"),
                self.config.get_local_auth_db(),
            ),
            "accounting": (
                os.path.join(restore_root, "tacacs_accounting.db"),
                self.config.get_database_config()["accounting_db"],
            ),
            "metrics": (
                os.path.join(restore_root, "metrics_history.db"),
                self.config.get_database_config()["metrics_history_db"],
            ),
            "audit": (
                os.path.join(restore_root, "audit_trail.db"),
                self.config.get_database_config()["audit_trail_db"],
            ),
            "overrides": (
                os.path.join(restore_root, "config_overrides.db"),
                "data/config_overrides.db",
            ),
        }

        databases_restored: list[str] = []

        if "config" in components:
            src_cfg = os.path.join(restore_root, "tacacs.conf")
            try:
                if os.path.exists(src_cfg) and getattr(
                    self.config, "config_file", None
                ):
                    os.makedirs(
                        os.path.dirname(self.config.config_file), exist_ok=True
                    )
                    shutil.copy2(src_cfg, self.config.config_file)
            except Exception as exc:
                _logger.warning("config_restore_failed", error=str(exc))
                # Restore overrides DB if present
                src_db, dst_db = db_map["overrides"]
                if os.path.exists(src_db):
                    self._restore_database(src_db, dst_db)
                    databases_restored.append(dst_db)

            for comp in ("devices", "users", "accounting", "metrics", "audit"):
                if comp in components:
                    src_db, dst_db = db_map[comp]
                    if os.path.exists(src_db):
                        self._restore_database(src_db, dst_db)
                        databases_restored.append(dst_db)

            # Step 6: Verify restored data
            for db_file in databases_restored:
                try:
                    self._verify_database_integrity(db_file)
                except Exception as exc:
                    _logger.error("db_integrity_failed", db=db_file, error=str(exc))
                    raise

            try:
                issues = self.config.validate_config()
                if issues:
                    raise ValueError("; ".join(issues))
            except Exception as e:
                _logger.error("restored_config_invalid", error=str(e))
                # A full rollback from emergency backup could be triggered here
                raise

            # Step 7: Log restore event
            try:
                if getattr(self.config, "config_store", None):
                    self.config.config_store.record_change(
                        section="system",
                        key="restore",
                        old_value=None,
                        new_value=source_path,
                        value_type="string",
                        changed_by="admin",
                        reason=f"Restored from backup: {manifest.get('backup_metadata', {}).get('timestamp_utc', '')}",
                    )
            except Exception:
                pass

            try:
                dur = (datetime.now(UTC) - t0).total_seconds()
                _logger.info(
                    json.dumps(
                        {
                            "event": "restore_completed",
                            "source_path": source_path,
                            "destination_id": destination_id,
                            "components": components or [],
                            "duration_seconds": dur,
                            "emergency_execution_id": emergency_exec_id,
                        }
                    )
                )
            except Exception:
                pass
            return True, "Restore completed"
        
        return False, "No components were restored"
        
    except Exception as exc:
        try:
            _logger.exception(
                json.dumps(
                    {
                        "event": "restore_failed",
                        "source_path": source_path,
                        "destination_id": destination_id,
                        "error": str(exc),
                        "error_type": type(exc).__name__,
                    }
                )
            )
        except Exception:
            pass
        return False, f"Restore failed: {exc}"
        
    finally:
        try:
            if restore_root and os.path.isdir(restore_root):
                shutil.rmtree(restore_root)
        except Exception:
            pass
            
        try:
            if local_archive and os.path.exists(local_archive):
                # Remove the main archive if it's a temporary download
                if destination_id:
                    os.remove(local_archive)
                
                # Remove decrypted temp if created
                if local_archive.endswith(".enc"):
                    dec_tmp = local_archive[:-4]  # Remove .enc
                    if os.path.exists(dec_tmp):
                        os.remove(dec_tmp)
        except Exception:
            pass

def create_manual_backup(self, destination_id: str, created_by: str) -> str:
    """Trigger manual backup and return execution_id."""
        raise ValueError("destination_id required")
    if not created_by:
        raise ValueError("created_by required")
    return self.execute_backup(
        destination_id, triggered_by=created_by, backup_type="manual"
    )
            raise ValueError("created_by required")
        return self.execute_backup(
            destination_id, triggered_by=created_by, backup_type="manual"
        )

    def get_backup_status(self, execution_id: str) -> dict:
        """Get status of an ongoing or completed backup by execution_id."""
            raise ValueError("execution_id required")
        return self.execution_store.get_execution(execution_id) or {}

    @staticmethod
    def _parse_json(v: Any) -> dict:
        import json

        if isinstance(v, dict):
            return v
        try:
            return json.loads(v) if isinstance(v, str) else {}
        except Exception:
            return {}

    # --- restore helpers ---
    @staticmethod
    def _extract_tarball(archive_path: str, dest_dir: str) -> str:
        """Securely extract tarball and return the top-level directory path."""
        # Create destination directory if it doesn't exist
        os.makedirs(dest_dir, exist_ok=True)
        
        # Extract the tarball
        with tarfile.open(archive_path, 'r:gz') as tar:
            # Get the top-level directory
            members = tar.getmembers()
            if not members:
                raise ValueError(f"Empty tarball: {archive_path}")
                
            # Extract all files
            tar.extractall(path=dest_dir)
            
            # Get the common prefix of all members
            common_prefix = os.path.commonprefix([m.name for m in members])
            
            # If there's a single top-level directory, return its path
            if common_prefix and os.path.exists(os.path.join(dest_dir, common_prefix, "manifest.json")):
                return os.path.join(dest_dir, common_prefix)
                
            # Otherwise, return the destination directory
            return dest_dir

    @staticmethod
    def _restore_database(src_db_path: str, dest_db_path: str) -> None:
        """Restore a database file with verification."""
        # Implementation for database restoration
        # This is a placeholder - the actual implementation should be added here
        raise NotImplementedError("Database restoration not implemented")

    def _pick_fallback_destination_id(self) -> str | None:
        try:
            rows = self.execution_store.list_destinations(enabled_only=True)
            if not rows:
                return None
            # Prefer local type if available
            for r in rows:
                if str(r.get("type", "")).lower() == "local":
                    return str(r.get("id"))
            return str(rows[0].get("id"))
        except Exception:
            return None

    @staticmethod
    def _get_encryption_passphrase() -> str | None:
        # Prefer environment variable to avoid storing secrets in DB
        return os.getenv("BACKUP_ENCRYPTION_PASSPHRASE") or None


# --- Service singleton (explicit init) ---
_backup_service_instance: BackupService | None = None


def initialize_backup_service(config: TacacsConfig) -> BackupService:
    """Initialize global backup service instance"""
    global _backup_service_instance
    execution_store = BackupExecutionStore("data/backup_executions.db")
    _backup_service_instance = BackupService(config, execution_store)
    try:
        _backup_service_instance.scheduler = BackupScheduler(_backup_service_instance)
        _backup_service_instance.scheduler.start()
    except Exception:
        pass
    return _backup_service_instance


def get_backup_service() -> 'BackupService':
    """Get global backup service instance."""
    if _backup_service_instance is None:
        raise RuntimeError("Backup service not initialized")
    return _backup_service_instance
